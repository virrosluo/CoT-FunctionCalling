{
    "use_peft": true,
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head"],
    "lora_r": 8,
    "lora_dropout": 0.1,
    "lora_alpha": 16,
    "lora_bias": "none",

    "use_quantize": true,
    "load_in_nbit": "4",
    "bnb_nbit_use_double_quant": true,
    "bnb_nbit_quant_type": "nf4",
    "bnb_nbit_compute_dtype": "bfloat16",

    "set_pad_token": "UNK",
    "use_fast_tokenizer": true,
    "tokenizer_trust_remote_code": true,

    "dataset_path": "Virros/CoT_Collection_Split",
    "dataset_trust_remote_code": true,

    "experiment": "mistral-cot",
    "response_start_template": [733, 28748, 16289, 28793],
    "neftune_noise": 5.0,

    "model_name_or_path": "mistralai/Mistral-7B-v0.1",
    "model_max_length": 2048,
    "cache_dir": ".",
    "token": true,
    "model_trust_remote_code": true,
    "low_cpu_mem_usage": true,
    "use_flash_attention_2": true,

    "output_dir": "./results",

    "num_train_epochs": 2,
    "per_device_train_batch_size": 6,
    "per_device_eval_batch_size": 8,
    "gradient_accumulation_steps": 1,
    "gradient_checkpointing": true,

    "do_train": true,
    "do_eval": true,

    "save_steps": 50,
    "logging_first_step": true,
    "logging_steps":50,
    "logging_strategy": "steps",
    "save_total_limit": 1,

    "load_best_model_at_end": true,

    "evaluation_strategy": "steps",

    "eval_accumulation_steps": 3,

    "remove_unused_columns": true,

    "optim": "paged_adamw_32bit",
    "learning_rate": 2e-4,
    "lr_scheduler_type": "cosine",
    "weight_decay": 0.001,
    "fp16": true,
    "bf16": false,
    "max_grad_norm": 0.3,
    "warmup_ratio": 0.03,
    "group_by_length": true,
    "report_to": ["wandb", "tensorboard"],

    "push_to_hub": false,
    "hub_model_id": "Virros/Mistral_CoT",
    "hub_strategy": "checkpoint",

    "run_name": "mistral_V1_cot_flashAttn2.0_neftune5",

    "overwrite_output_dir": false
}